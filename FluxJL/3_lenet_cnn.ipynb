{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flux JL\n",
    "\n",
    "## Flux Model Zoo Examples\n",
    "\n",
    "# 3. LeNet CNN\n",
    "\n",
    "**FluxML contributors**\n",
    "\n",
    "**Source:** https://github.com/FluxML/model-zoo/blob/master/vision/conv_mnist/conv_mnist.jl\n",
    "\n",
    "In this notebook we will do a Julia version of the LeNet5 convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling ProgressMeter [92933f4c-e287-5a05-a399-4b506db050ca]\n",
      "└ @ Base loading.jl:1278\n"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "using Flux.Data: DataLoader\n",
    "using Flux.Optimise: Optimiser, WeightDecay\n",
    "using Flux: onehotbatch, onecold\n",
    "using Flux.Losses: logitcrossentropy\n",
    "using Statistics, Random\n",
    "using MLDatasets: MNIST\n",
    "using ProgressMeter: @showprogress\n",
    "using Logging: with_logger\n",
    "using TensorBoardLogger: TBLogger, tb_overwrite, set_step!, set_step_increment!\n",
    "using CUDA\n",
    "import BSON\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params(model) = sum(length, Flux.params(model)) \n",
    "round4(x) = round(x, digits=4)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet5\n",
    "\n",
    "Let's start with the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "LeNet5 (generic function with 1 method)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function LeNet5(; imgdims=(28,28,1), nclasses=10)\n",
    "    outer_conv_kernel_size = (\n",
    "        imgdims[1]÷4 - 3,  # formula based on stride 1 and padding 0\n",
    "        imgdims[2]÷4 - 3,\n",
    "        16\n",
    "    )\n",
    "\n",
    "    return Chain(\n",
    "            Conv((5,5), imgdims[end]=>6, relu),   # Flux.Conv 2nd argument is the pair: n_in_channels => n_out_channels\n",
    "            MaxPool((2, 2)),\n",
    "            Conv((5,5), 6=>16, relu),\n",
    "            MaxPool((2, 2)),\n",
    "            flatten,\n",
    "            Dense(prod(outer_conv_kernel_size), 120, relu),\n",
    "            Dense(120, 84, relu),\n",
    "            Dense(84, nclasses)\n",
    "        )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "\n",
    "We will use MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "get_data (generic function with 1 method)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_data(batchsize)\n",
    "    xtrain, ytrain = MNIST.traindata(Float32)\n",
    "    xtest, ytest = MNIST.testdata(Float32)\n",
    "\n",
    "    xtrain = reshape(xtrain, 28, 28, 1, :)\n",
    "    xtest = reshape(xtest, 28, 28, 1, :)\n",
    "\n",
    "    ytrain = onehotbatch(ytrain, 0:9)\n",
    "    ytest = onehotbatch(ytest, 0:9)\n",
    "\n",
    "    train_loader = DataLoader((xtrain, ytrain), batchsize=batchsize, shuffle=true)\n",
    "    test_loader = DataLoader((xtest, ytest),  batchsize=batchsize)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "loss (generic function with 1 method)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(ŷ, y) = logitcrossentropy(ŷ, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "evaluate (generic function with 1 method)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function evaluate(dataloader, model, device)\n",
    "    l_counter = 0f0\n",
    "    accuracy_counter = 0\n",
    "    n_counter = 0\n",
    "\n",
    "    for (x,y) in dataloader\n",
    "        x, y = x |> device, y |> device\n",
    "\n",
    "        ŷ = model(x)\n",
    "\n",
    "        l_counter += loss(ŷ, y) * size(x)[end]     \n",
    "        accuracy_counter += sum(onecold(ŷ |> cpu) .== onecold(y |> cpu))\n",
    "        n_counter += size(x)[end]\n",
    "    end\n",
    "    \n",
    "    eval_loss = l_counter / n_counter |> round4\n",
    "    eval_accuracy = 100 * (accuracy_counter / n_counter) |> round4\n",
    "\n",
    "    return (loss=eval_loss, accuracy=eval_accuracy)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "train (generic function with 1 method)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(; kws...)\n",
    "    args = Args(; kws...)\n",
    "    args.seed > 0 && Random.seed!(args.seed)\n",
    "    use_cuda = args.use_cuda && CUDA.functional()\n",
    "\n",
    "    \n",
    "    if use_cuda\n",
    "        device = gpu\n",
    "        @info \"Training on GPU\"\n",
    "    else\n",
    "        device = cpu\n",
    "        @info \"Training on CPU\"\n",
    "    end\n",
    "\n",
    "    ## Data\n",
    "    train_loader, test_loader = get_data(args.batchsize)\n",
    "    @info \"Dataset MNIST: $(train_loader.nobs) train and $(test_loader.nobs) test examples\"\n",
    "\n",
    "    ## Model\n",
    "    model = LeNet5() |> device\n",
    "    @info \"LeNet5 model: $(num_params(model)) trainable params\" \n",
    "    \n",
    "    ## Optimiser\n",
    "    θ = Flux.params(model)\n",
    "\n",
    "    optimiser = ADAM(args.η)\n",
    "    if args.λ > 0 # add weight decay, ie. L2 reguralirization\n",
    "        optimiser = Optimiser(optimiser, WeightDecay(args.λ))\n",
    "    end\n",
    "    \n",
    "    ## TensorBoard Logger\n",
    "    if args.tensorboard \n",
    "        tblogger = TBLogger(args.savepath, tb_overwrite)\n",
    "        set_step_increment!(tblogger, 0) # 0 auto increment since we manually set_step!\n",
    "        @info \"TensorBoard logging at \\\"$(args.savepath)\\\"\"\n",
    "    end\n",
    "\n",
    "    ## Epoch logging\n",
    "    function report(epoch)\n",
    "        train = evaluate(train_loader, model, device)\n",
    "        test = evaluate(test_loader, model, device)        \n",
    "        println(\"Epoch: $epoch   Train: $(train)   Test: $(test)\")\n",
    "        if args.tensorboard\n",
    "            set_step!(tblogger, epoch)\n",
    "            with_logger(tblogger) do\n",
    "                @info \"train\" loss=train.loss  acc=train.accuracy\n",
    "                @info \"test\"  loss=test.loss   acc=test.accuracy\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    ## Training Loop\n",
    "    @info \"Training started ...\"\n",
    "    report(0)\n",
    "    for epoch in 1:args.epochs\n",
    "        @showprogress for (x,y) in train_loader\n",
    "            x, y = x |> device, y |> device\n",
    "            ∂loss = Flux.gradient(θ) do\n",
    "                        ŷ = model(x)\n",
    "                        loss(ŷ, y)\n",
    "                    end\n",
    "            \n",
    "            Flux.Optimise.update!(optimiser, θ, ∂loss)\n",
    "        end\n",
    "\n",
    "        ## Printing and logging\n",
    "        epoch % args.infotime == 0 && report(epoch)\n",
    "        if args.checktime > 0 && epoch % args.checktime == 0\n",
    "            !ispath(args.savepath) && mkpath(args.savepath)\n",
    "            modelpath = joinpath(args.savepath, \"model.bson\") \n",
    "            let model = cpu(model) #return model to cpu before serialization\n",
    "                BSON.@save modelpath model epoch\n",
    "            end\n",
    "            @info \"Model saved in \\\"$(modelpath)\\\"\"\n",
    "        end\n",
    "    end\n",
    "end\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LeNet5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programme Parameters\n",
    "\n",
    "Arguments for the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Args"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Base.@kwdef mutable struct Args\n",
    "    η = 3e-4             # learning rate\n",
    "    λ = 0                # L2 regularizer param, implemented as weight decay\n",
    "    batchsize = 128      # batch size\n",
    "    epochs = 10          # number of epochs\n",
    "    seed = 0             # set seed > 0 for reproducibility\n",
    "    use_cuda = false      # if true use cuda (if available)\n",
    "    infotime = 1 \t     # report every `infotime` epochs\n",
    "    checktime = 5        # Save the model every `checktime` epochs. Set to 0 for no checkpoints.\n",
    "    tensorboard = false      # log training with tensorboard\n",
    "    savepath = \"runs/lenet5\"    # results path\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training on CPU\n",
      "└ @ Main In[29]:12\n",
      "┌ Info: Dataset MNIST: 60000 train and 10000 test examples\n",
      "└ @ Main In[29]:17\n",
      "┌ Info: LeNet5 model: 44426 trainable params\n",
      "└ @ Main In[29]:21\n",
      "┌ Info: TensorBoard logging at \"runs/lenet5\"\n",
      "└ @ Main In[29]:35\n",
      "┌ Info: Training started ...\n",
      "└ @ Main In[29]:53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0   Train: (loss = 2.3028f0, accuracy = 11.4017)   Test: (loss = 2.3019f0, accuracy = 11.51)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:32\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1   Train: (loss = 0.1819f0, accuracy = 94.5933)   Test: (loss = 0.1659f0, accuracy = 95.13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  56%|███████████████████████▏                 |  ETA: 0:00:14\u001b[39m"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "name": "julia",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}