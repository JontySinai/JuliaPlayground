{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Julia Academy\n",
    "\n",
    "## Data Science Course\n",
    "\n",
    "# 6. Classical Classification Algorithms in Julia\n",
    "\n",
    "**Huda Nassar**\n",
    "\n",
    "**Source:** https://github.com/JuliaAcademy/DataScience/blob/main/06.%20Classification.ipynb\n",
    "\n",
    "We will use different implementations of classical classification algorithms in Julia and compare them at the end. We will use the classic iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling GLMNet [8d5ece8b-de18-5317-b113-243142960cc6]\n",
      "└ @ Base loading.jl:1278\n",
      "┌ Info: Precompiling DecisionTree [7806a523-6efd-50cb-b5f6-3fa6f1930dbb]\n",
      "└ @ Base loading.jl:1278\n",
      "┌ Info: Precompiling LIBSVM [b1bec4e5-fd48-53fe-b0cb-9723c09d164b]\n",
      "└ @ Base loading.jl:1278\n"
     ]
    }
   ],
   "source": [
    "using GLMNet\n",
    "using RDatasets\n",
    "using MLBase\n",
    "using Plots\n",
    "using DecisionTree\n",
    "using Distances\n",
    "using NearestNeighbors\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "using DataStructures\n",
    "using LIBSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV[\"LINES\"], ENV[\"COLUMNS\"] = 15, 200;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a simple function for computing the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "compute_accuracy (generic function with 1 method)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(predicted, groundtruth) = sum(predicted .== groundtruth) / length(groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<table class=\"data-frame\"><thead><tr><th></th><th>SepalLength</th><th>SepalWidth</th><th>PetalLength</th><th>PetalWidth</th><th>Species</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Cat…</th></tr></thead><tbody><p>150 rows × 5 columns</p><tr><th>1</th><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>setosa</td></tr><tr><th>2</th><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>setosa</td></tr><tr><th>3</th><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>setosa</td></tr><tr><th>4</th><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>setosa</td></tr><tr><th>5</th><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>setosa</td></tr><tr><th>6</th><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>setosa</td></tr><tr><th>7</th><td>4.6</td><td>3.4</td><td>1.4</td><td>0.3</td><td>setosa</td></tr><tr><th>8</th><td>5.0</td><td>3.4</td><td>1.5</td><td>0.2</td><td>setosa</td></tr><tr><th>9</th><td>4.4</td><td>2.9</td><td>1.4</td><td>0.2</td><td>setosa</td></tr><tr><th>10</th><td>4.9</td><td>3.1</td><td>1.5</td><td>0.1</td><td>setosa</td></tr><tr><th>11</th><td>5.4</td><td>3.7</td><td>1.5</td><td>0.2</td><td>setosa</td></tr><tr><th>12</th><td>4.8</td><td>3.4</td><td>1.6</td><td>0.2</td><td>setosa</td></tr><tr><th>13</th><td>4.8</td><td>3.0</td><td>1.4</td><td>0.1</td><td>setosa</td></tr><tr><th>14</th><td>4.3</td><td>3.0</td><td>1.1</td><td>0.1</td><td>setosa</td></tr><tr><th>15</th><td>5.8</td><td>4.0</td><td>1.2</td><td>0.2</td><td>setosa</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>",
      "text/latex": "\\begin{tabular}{r|ccccc}\n\t& SepalLength & SepalWidth & PetalLength & PetalWidth & Species\\\\\n\t\\hline\n\t& Float64 & Float64 & Float64 & Float64 & Cat…\\\\\n\t\\hline\n\t1 & 5.1 & 3.5 & 1.4 & 0.2 & setosa \\\\\n\t2 & 4.9 & 3.0 & 1.4 & 0.2 & setosa \\\\\n\t3 & 4.7 & 3.2 & 1.3 & 0.2 & setosa \\\\\n\t4 & 4.6 & 3.1 & 1.5 & 0.2 & setosa \\\\\n\t5 & 5.0 & 3.6 & 1.4 & 0.2 & setosa \\\\\n\t6 & 5.4 & 3.9 & 1.7 & 0.4 & setosa \\\\\n\t7 & 4.6 & 3.4 & 1.4 & 0.3 & setosa \\\\\n\t8 & 5.0 & 3.4 & 1.5 & 0.2 & setosa \\\\\n\t9 & 4.4 & 2.9 & 1.4 & 0.2 & setosa \\\\\n\t10 & 4.9 & 3.1 & 1.5 & 0.1 & setosa \\\\\n\t11 & 5.4 & 3.7 & 1.5 & 0.2 & setosa \\\\\n\t12 & 4.8 & 3.4 & 1.6 & 0.2 & setosa \\\\\n\t13 & 4.8 & 3.0 & 1.4 & 0.1 & setosa \\\\\n\t14 & 4.3 & 3.0 & 1.1 & 0.1 & setosa \\\\\n\t15 & 5.8 & 4.0 & 1.2 & 0.2 & setosa \\\\\n\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n\\end{tabular}\n",
      "text/plain": "\u001b[1m150×5 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m SepalLength \u001b[0m\u001b[1m SepalWidth \u001b[0m\u001b[1m PetalLength \u001b[0m\u001b[1m PetalWidth \u001b[0m\u001b[1m Species   \u001b[0m\n\u001b[1m     \u001b[0m│\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Cat…      \u001b[0m\n─────┼─────────────────────────────────────────────────────────────\n   1 │         5.1         3.5          1.4         0.2  setosa\n   2 │         4.9         3.0          1.4         0.2  setosa\n   3 │         4.7         3.2          1.3         0.2  setosa\n  ⋮  │      ⋮           ⋮            ⋮           ⋮           ⋮\n 148 │         6.5         3.0          5.2         2.0  virginica\n 149 │         6.2         3.4          5.4         2.3  virginica\n 150 │         5.9         3.0          5.1         1.8  virginica\n\u001b[36m                                                   144 rows omitted\u001b[0m"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = dataset(\"datasets\", \"iris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict the species from the measured feature variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Matrix(iris[:, 1:4])  # feature matrix\n",
    "irislabels = iris[:, 5];   # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "3-element Array{String,1}:\n \"setosa\"\n \"versicolor\"\n \"virginica\""
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique(irislabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to encode the labels. We can use MLBase to do this easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "150-element Array{Int64,1}:\n 1\n 1\n 1\n 1\n 1\n ⋮\n 3\n 3\n 3\n 3\n 3"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irislabels_map = MLBase.labelmap(irislabels)\n",
    "y = MLBase.labelencode(irislabels_map, irislabels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "get_class_training_indices (generic function with 1 method)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_class_training_indices(labels, train_ratio)\n",
    "    uids = unique(labels)\n",
    "    keepids = []\n",
    "    for label in uids\n",
    "        label_indices = findall(labels.==label)\n",
    "        label_train_indices = randsubseq(label_indices, train_ratio)\n",
    "        push!(keepids, label_train_indices...)\n",
    "    end\n",
    "    return keepids\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 111\n",
      "Number of test samples: 39\n"
     ]
    }
   ],
   "source": [
    "train_idx = get_class_training_indices(y, 0.7)\n",
    "test_idx = setdiff(1:length(y), train_idx)\n",
    "\n",
    "println(\"Number of training samples: \", length(train_idx))\n",
    "println(\"Number of test samples: \", length(test_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will need a function to assign class labels based on the continuous outputs of the GLMNet classification algorithms we'll use. The outputs will be real-valued near the integer class ID's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "assign_class (generic function with 1 method)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assign_class(predicted_output) = argmin(abs.(predicted_output .- [1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: The Lasso\n",
    "\n",
    "We will use logistic regression with L1 reguralirization using a Julia wrapper of Trevor Hastie et al's [glmnet](https://www.jstatsoft.org/article/view/v033i01) package.\n",
    "\n",
    "glmnet runs a generic regression for different values of parameter \\lambda. These runs are done along a so-called path.\n",
    "\n",
    "> **Note:** glmnet also has a parameter \\alpha which controls whether we use ridge regression (L2 norm) or lasso (L1 norm). By default \\alpha != 1 implies using Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Least Squares GLMNet Solution Path (70 solutions for 4 predictors in 1089 passes):\n──────────────────────────────\n      df   pct_dev           λ\n──────────────────────────────\n [1]   0  0.0       0.789924\n [2]   2  0.156264  0.719749\n [3]   2  0.28783   0.655809\n [4]   2  0.397058  0.597549\n [5]   2  0.48774   0.544464\n [6]   2  0.563025  0.496095\n [7]   2  0.625526  0.452024\n [8]   2  0.677415  0.411867\n [9]   2  0.720493  0.375278\n[10]   2  0.756256  0.341939\n[11]   2  0.785946  0.311562\n[12]   2  0.810595  0.283884\n[13]   2  0.831059  0.258664\n[14]   2  0.848047  0.235685\n[15]   2  0.862151  0.214748\n[16]   2  0.87386   0.19567\n[17]   2  0.88358   0.178287\n[18]   2  0.89165   0.162449\n[19]   2  0.898345  0.148017\n[20]   2  0.903906  0.134868\n[21]   2  0.908523  0.122887\n[22]   2  0.912357  0.11197\n[23]   2  0.915539  0.102023\n[24]   2  0.918178  0.0929592\n[25]   2  0.920371  0.084701\n[26]   2  0.922191  0.0771764\n[27]   2  0.923703  0.0703202\n[28]   2  0.924956  0.0640732\n[29]   3  0.926245  0.0583811\n[30]   3  0.927436  0.0531947\n[31]   3  0.928425  0.048469\n[32]   3  0.929247  0.0441632\n[33]   3  0.92993   0.0402398\n[34]   3  0.930496  0.036665\n[35]   3  0.930966  0.0334078\n[36]   3  0.931357  0.0304399\n[37]   3  0.931681  0.0277357\n[38]   3  0.93195   0.0252718\n[39]   3  0.932173  0.0230267\n[40]   3  0.932359  0.0209811\n[41]   3  0.932513  0.0191172\n[42]   3  0.932641  0.0174189\n[43]   3  0.932748  0.0158714\n[44]   3  0.932836  0.0144614\n[45]   3  0.932909  0.0131767\n[46]   3  0.93297   0.0120061\n[47]   4  0.933027  0.0109395\n[48]   4  0.933517  0.00996771\n[49]   4  0.934074  0.00908221\n[50]   4  0.934547  0.00827537\n[51]   4  0.934936  0.00754021\n[52]   4  0.935263  0.00687036\n[53]   4  0.935536  0.00626001\n[54]   4  0.935763  0.00570389\n[55]   4  0.935952  0.00519717\n[56]   4  0.936109  0.00473547\n[57]   4  0.93624   0.00431478\n[58]   4  0.936349  0.00393147\n[59]   4  0.936439  0.00358221\n[60]   4  0.936515  0.00326398\n[61]   4  0.936577  0.00297401\n[62]   4  0.936629  0.00270981\n[63]   4  0.936674  0.00246908\n[64]   4  0.936711  0.00224973\n[65]   4  0.936741  0.00204987\n[66]   4  0.936767  0.00186777\n[67]   4  0.936789  0.00170184\n[68]   3  0.936805  0.00155065\n[69]   3  0.936816  0.0014129\n[70]   3  0.936824  0.00128738\n──────────────────────────────"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = glmnet(X[train_idx, :], y[train_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we choose the correct value of \\lambda? We can use cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Least Squares GLMNet Cross Validation\n70 models for 4 predictors in 10 folds\nBest λ 0.002 (mean loss 0.047, std 0.008)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_folds = glmnetcv(X[train_idx, :], y[train_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can select the best lambda from the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.0018677669431407396"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "λ = path.lambda[argmin(cv_folds.meanloss)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will set the best lambda and run a training pass using this parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Least Squares GLMNet Solution Path (1 solutions for 4 predictors in 65 passes):\n─────────────────────────────\n     df   pct_dev           λ\n─────────────────────────────\n[1]   3  0.936799  0.00186777\n─────────────────────────────"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lasso_path = glmnet(X[train_idx, :], y[train_idx], lambda=[λ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Predictions\n",
    "\n",
    "Now we can use the best path to get the test predictions using the Lasso algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "39×1 Array{Float64,2}:\n 0.9513267362335774\n 1.0223467910449973\n 0.9663341636378728\n 0.9134379730064015\n 0.8979284103258488\n ⋮\n 2.883368271870407\n 2.6727877798970363\n 2.754192419508327\n 3.0104009540889516\n 2.7049148009287656"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query = X[test_idx, :]\n",
    "\n",
    "predictions_lasso = GLMNet.predict(best_lasso_path, test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "39×1 Array{Int64,2}:\n 1\n 1\n 1\n 1\n 1\n ⋮\n 3\n 3\n 3\n 3\n 3"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_lasso = assign_class.(predictions_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.9743589743589743"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(predictions_lasso, y[test_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Ridge Regression\n",
    "\n",
    "If we want ridge regression, we can set glmnet's \\alpha parameter to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Least Squares GLMNet Solution Path (1 solutions for 4 predictors in 29 passes):\n────────────────────────────\n     df   pct_dev          λ\n────────────────────────────\n[1]   4  0.928539  0.0789924\n────────────────────────────"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_path = glmnet(X[train_idx, :], y[train_idx], alpha=0)\n",
    "ridge_cv_folds = glmnetcv(X[train_idx, :], y[train_idx], alpha=0)\n",
    "λ_ridge = ridge_path.lambda[argmin(ridge_cv_folds.meanloss)]\n",
    "\n",
    "best_ridge_path = glmnet(X[train_idx, :], y[train_idx], alpha=0, lambda=[λ_ridge])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_ridge = GLMNet.predict(best_ridge_path, test_query)\n",
    "predictions_ridge = assign_class.(predictions_ridge)\n",
    "compute_accuracy(predictions_ridge, y[test_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Elastic Net\n",
    "\n",
    "Elastic net is a weighted combination of Ridge and Lasso regression. We will use \\alpha=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Least Squares GLMNet Solution Path (1 solutions for 4 predictors in 70 passes):\n────────────────────────────\n     df  pct_dev           λ\n────────────────────────────\n[1]   3  0.93684  0.00177468\n────────────────────────────"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_path = glmnet(X[train_idx, :], y[train_idx], alpha=0.5)\n",
    "elastic_cv_folds = glmnetcv(X[train_idx, :], y[train_idx], alpha=0.5)\n",
    "λ_elastic = elastic_path.lambda[argmin(elastic_cv_folds.meanloss)]\n",
    "\n",
    "best_elastic_path = glmnet(X[train_idx, :], y[train_idx], alpha=0.5, lambda=[λ_elastic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.9743589743589743"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_elastic = GLMNet.predict(best_elastic_path, test_query)\n",
    "predictions_elastic = assign_class.(predictions_elastic)\n",
    "compute_accuracy(predictions_elastic, y[test_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4: Decision Trees\n",
    "\n",
    "We will use the `DecisionTree.jl` package to work with decision tree algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "DecisionTreeClassifier\nmax_depth:                2\nmin_samples_leaf:         1\nmin_samples_split:        2\nmin_purity_increase:      0.0\npruning_purity_threshold: 1.0\nn_subfeatures:            0\nclasses:                  [1, 2, 3]\nroot:                     Decision Tree\nLeaves: 3\nDepth:  2"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_DT = DecisionTreeClassifier(max_depth=2)\n",
    "DecisionTree.fit!(model_DT, X[train_idx, :], y[train_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will query the tree on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.9743589743589743"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_DT = DecisionTree.predict(model_DT, test_query)\n",
    "compute_accuracy(predictions_DT, y[test_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 5: Random Forest\n",
    "\n",
    "Decision tree classifiers can be scaled to random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestClassifier\nn_trees:             20\nn_subfeatures:       -1\npartial_sampling:    0.7\nmax_depth:           -1\nmin_samples_leaf:    1\nmin_samples_split:   2\nmin_purity_increase: 0.0\nclasses:             [1, 2, 3]\nensemble:            Ensemble of Decision Trees\nTrees:      20\nAvg Leaves: 6.8\nAvg Depth:  4.9"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_RF = RandomForestClassifier(n_trees=20)\n",
    "DecisionTree.fit!(model_RF, X[train_idx, :], y[train_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.9743589743589743"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_RF = DecisionTree.predict(model_RF, test_query)\n",
    "compute_accuracy(predictions_RF, y[test_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 6: Nearest Neighbors\n",
    "\n",
    "Now we will use the nearest neighbor method for classifying. The algorithm is similar to kmeans but the details are different.\n",
    "\n",
    "We will start by building a k-nearest neighbours look-up table based on the training input data only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "KDTree{StaticArrays.SArray{Tuple{4},Float64,1,4},Euclidean,Float64}\n  Number of points: 111\n  Dimensions: 4\n  Metric: Euclidean(0.0)\n  Reordered: true"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdtree = KDTree(X[train_idx, :]')  # we use the transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will map the 5 nearest training samples to each test sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs, dists = knn(kdtree, test_query', 5, true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will use the above indices and distances for each test sample to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "5×39 Array{Int64,2}:\n 1  1  1  1  1  1  1  1  1  2  2  2  2  2  2  3  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3\n 1  1  1  1  1  1  1  1  1  2  2  2  2  2  2  3  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3\n 1  1  1  1  1  1  1  1  1  2  2  2  2  2  3  2  2  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3  3  3  3  3\n 1  1  1  1  1  1  1  1  1  2  2  2  2  2  2  3  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3  3  2  3  3  2\n 1  1  1  1  1  1  1  1  1  2  2  2  3  2  3  2  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3  3  2  3  3  3"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN_classes = y[train_idx][hcat(idxs...)]   # 5 nearest neighbours x n_test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "39-element Array{Accumulator{Int64,Int64},1}:\n Accumulator(1 => 5)\n Accumulator(1 => 5)\n Accumulator(1 => 5)\n Accumulator(1 => 5)\n Accumulator(1 => 5)\n ⋮\n Accumulator(3 => 5)\n Accumulator(2 => 2, 3 => 3)\n Accumulator(3 => 5)\n Accumulator(3 => 5)\n Accumulator(2 => 1, 3 => 4)"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_labels = map(i -> counter(KNN_classes[:, i]), 1:size(KNN_classes, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.9743589743589743"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_KNN = map(i -> argmax(possible_labels[i]), 1:size(KNN_classes,2))\n",
    "compute_accuracy(predictions_KNN, y[test_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 7: Support Vector Machines\n",
    "\n",
    "The last classical method we'll look at is Support Vector Machines. We'll use of course LIBSVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "LIBSVM.SVM{Int64}(SVC, LIBSVM.Kernel.RadialBasis, nothing, 4, 3, [1, 2, 3], Int32[1, 2, 3], Float64[], Int32[], LIBSVM.SupportVectors{Int64,Float64}(35, Int32[5, 15, 15], [1, 1, 1, 1, 1, 2, 2, 2, 2, 2  …  3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [5.7 5.7 … 6.9 5.8; 4.4 3.8 … 3.1 2.7; 1.5 1.7 … 5.1 5.1; 0.4 0.3 … 2.3 1.9], Int32[11, 13, 18, 33, 36, 42, 43, 44, 46, 47  …  93, 96, 97, 99, 101, 103, 104, 107, 109, 110], LIBSVM.SVMNode[LIBSVM.SVMNode(1, 5.7), LIBSVM.SVMNode(1, 5.7), LIBSVM.SVMNode(1, 4.8), LIBSVM.SVMNode(1, 4.5), LIBSVM.SVMNode(1, 5.1), LIBSVM.SVMNode(1, 7.0), LIBSVM.SVMNode(1, 6.4), LIBSVM.SVMNode(1, 6.9), LIBSVM.SVMNode(1, 6.3), LIBSVM.SVMNode(1, 4.9)  …  LIBSVM.SVMNode(1, 6.3), LIBSVM.SVMNode(1, 6.2), LIBSVM.SVMNode(1, 6.1), LIBSVM.SVMNode(1, 7.2), LIBSVM.SVMNode(1, 7.9), LIBSVM.SVMNode(1, 6.3), LIBSVM.SVMNode(1, 6.1), LIBSVM.SVMNode(1, 6.0), LIBSVM.SVMNode(1, 6.9), LIBSVM.SVMNode(1, 5.8)]), 0.0, [0.4402103896003432 0.9389584892572648; 0.06574646141834749 0.0; … ; -0.1377420052095731 -0.9778055558853347; -0.0 -1.0], Float64[], Float64[], [0.040023573380045696, 0.14935212474468545, 0.239532199093954], 3, 0.25, 200.0, 0.001, 1.0, 0.5, 0.1, true, false)"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svm = LIBSVM.svmtrain(X[train_idx, :]', y[train_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.9743589743589743"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_SVM, decision_values = LIBSVM.svmpredict(model_svm, test_query')\n",
    "compute_accuracy(predictions_SVM, y[test_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting Everything Together\n",
    "\n",
    "Now we will compare the accuracy of all the methods above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "7×2 Array{Any,2}:\n \"Lasso\"          0.974359\n \"Ridge\"          1.0\n \"Elastic Net\"    0.974359\n \"Decision Tree\"  0.974359\n \"Random Forest\"  0.974359\n \"KNN\"            0.974359\n \"SVM\"            0.974359"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_accuracies = zeros(7)\n",
    "methods = [\"Lasso\", \"Ridge\", \"Elastic Net\", \"Decision Tree\", \"Random Forest\", \"KNN\", \"SVM\"]\n",
    "\n",
    "groundtruth = y[test_idx]\n",
    "\n",
    "overall_accuracies[1] = compute_accuracy(predictions_lasso, groundtruth)\n",
    "overall_accuracies[2] = compute_accuracy(predictions_ridge, groundtruth)\n",
    "overall_accuracies[3] = compute_accuracy(predictions_elastic, groundtruth)\n",
    "overall_accuracies[4] = compute_accuracy(predictions_DT, groundtruth)\n",
    "overall_accuracies[5] = compute_accuracy(predictions_RF, groundtruth)\n",
    "overall_accuracies[6] = compute_accuracy(predictions_KNN, groundtruth)\n",
    "overall_accuracies[7] = compute_accuracy(predictions_SVM, groundtruth)\n",
    "\n",
    "hcat(methods, overall_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Ridge Regression ends up being the best method for this dataset and the rest have equivalent accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}